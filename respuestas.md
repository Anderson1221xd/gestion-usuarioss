1. Identificaci√≥n de fallos en los logs

Durante la ejecuci√≥n del pipeline de CI, los logs permiten identificar claramente en qu√© etapa ocurri√≥ un fallo. Estas son las se√±ales para cada caso:

üîπ 1.1 Fallos del Linter (Google Java Format)

Los errores del linter aparecen usualmente con mensajes como:

"Code style violation"

"Formatting error"

"Google Java Format found files that need formatting"

Adem√°s:

El job del linter termina con exit code 1.

El pipeline se detiene inmediatamente.

C√≥mo detectarlo en los logs:

Buscar palabras clave: format, style, google-java-format, error.

Ver que el workflow no avanza a la etapa de build.

üîπ 1.2 Fallos en Pruebas Unitarias (JUnit)

Cuando falla una prueba, JUnit muestra:

Nombre de la prueba fallida

El error lanzado (AssertionError, NullPointerException, etc.)

N√∫mero de pruebas exitosas y fallidas

Ejemplo t√≠pico en logs:

Tests run: 4, Failures: 1, Errors: 0, Skipped: 0


El pipeline falla autom√°ticamente.

üîπ 1.3 Fallos por Cobertura (JaCoCo)

El plugin de JaCoCo valida un m√≠nimo definido (en este caso, 80%).

Si est√° por debajo, ver√°s mensajes como:

"Rule violated for class X: coverage is below threshold"

"Coverage check failed"

Los logs muestran:

[ERROR] Coverage percentage (75%) is below the minimum required (80%)


El job termina con exit code 1.

2. Ejecuci√≥n fallida vs exitosa
üîπ 2.1 Ejecuci√≥n Fallida (obligatoria para el parcial)

Para generar un run fallido se puede:

Romper el linter (a√±adir espacios indebidos o desordenar llaves)

Romper las pruebas (assert incorrecto)

Bajar cobertura (comentar una prueba)

En el log de un run fallido se observa:

El job se pinta en rojo

Mensajes de error visibles (JUnit, JaCoCo o linter)

El pipeline se detiene en ese punto

üîπ 2.2 Ejecuci√≥n Exitosa

Para generar un run exitoso:

C√≥digo formateado correctamente

Todas las pruebas pasan

Cobertura ‚â• 80%

En los logs se observar√°:

Todos los pasos en verde

Salida final: "Job succeeded"

JaCoCo muestra cobertura aprobada

Maven indica: "BUILD SUCCESS"

3. M√©todos para detectar c√≥digo generado por IA
3.1 Detector basado en patrones ling√º√≠sticos

Herramientas como GPTZero, CodeWhisperer Detector, Copyleaks AI Detector analizan:

Fluidez excesivamente uniforme

Complejidad sint√°ctica repetitiva

Ausencia de errores humanos t√≠picos

Estructura de c√≥digo altamente sim√©trica

Funcionan midiendo la perplejidad: qu√© tan ‚Äúpredecible‚Äù es el texto.

3.2 An√°lisis estad√≠stico del estilo de programaci√≥n

M√©todos utilizados:

Comparaci√≥n del estilo entre archivos del mismo autor

Frecuencias de patrones (indentaci√≥n, nombres repetitivos, comentarios uniformes)

M√©tricas de complejidad ciclom√°tica extra√±amente homog√©nea

Muchos detectores combinan este an√°lisis con machine learning.

4. Por qu√© no es posible asegurar la autor√≠a al 100%

IA y humanos pueden producir c√≥digo indistinguible

Un estudiante puede mejorar su estilo y parecer IA

Un programador puede pedir ayuda a IA parcialmente

Detectores de IA generan falsos positivos y falsos negativos

No existe una firma digital que pruebe la autor√≠a humana

Por eso ning√∫n m√©todo puede garantizar autor√≠a total.

5. Pol√≠ticas razonables para el uso de IA en educaci√≥n y calidad

Permitir IA para documentaci√≥n, explicaciones y debugging, pero no para generar c√≥digo en evaluaciones.

Declarar cuando se utiliza IA, igual que una referencia bibliogr√°fica.

Uso libre en proyectos reales, siempre que exista revisi√≥n del desarrollador.

Prohibir IA en ex√°menes y parciales, salvo autorizaci√≥n expl√≠cita.

Ense√±ar pensamiento cr√≠tico antes que dependencia en IA, para fortalecer competencias reales.
